{
  "hash": "19743b090bf4fc85d6b76098bae984d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"tidymodels-进阶\"\nauthor: \"一把Fei刀\"\ndate: 2024-2-11\ndate-modified: last-modified\nnumber-sections: true\nnotebook-view: \n  - notebook: \"../notebooks/tidymodels_2.qmd\"\n    title: \"完整代码笔记本\"\nnotebook-links: global\n---\n\n\n\n\n## 重采样方法\n\n\n::: {.cell}\n\n:::\n\n```{mermaid}\n%%| label: fig-mermaid_1\n%%| echo: false\n---\ntitle: 重采样方法的数据分割\n---\nflowchart TD\nA(原始数据) --> B[[训练集]]\nA --> C[[测试集]]\nB --> D{{重采样1}}\nB --> E{{重采样2}}\nB --> F{{重采样n}}\nD --> G[[分析集]]\nD --> H[[评估集]]\nE --> I[[分析集]]\nE --> J[[评估集]]\nF --> K[[分析集]]\nF --> L[[评估集]]\nD -.- E\nE -.- F\n```\n\n\n偏倚*bias*：数据中的真实模式或关系与模型所能模拟的模式类型之间的差异。运用合理的重采样方法，可以有效减小偏倚。\n\n重采样只在训练集上进行，测试集不参与重采样。每次重采样，训练集都会被划分为分析集和评估集。分析集用于训练模型，评估集用于评估模型的性能，如 @fig-mermaid_1 所示。模型性能的最终估计值是所有评估集重复统计的平均值。\n\n### 交叉验证CV\n\n#### k折交叉验证k-CV {.unnumbered}\n\n交叉验证是一种重采样方法，最常见的是k折交叉验证。在k折交叉验证中，数据被分为k个子集，其中一个子集被保留作为评估集，其余k-1个子集被用于训练模型。这个过程重复k次，每个子集都有一次机会作为评估集。最后，模型性能的最终估计值是所有评估集重复统计的平均值。\n\n`vfold_cv()`函数可以用于创建k折交叉验证。\n\n:::callout-important\nk值越大，重取样估计值偏倚越小，但方差越大。通常k值取5或10。\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntidymodels_prefer()\n\n# 10折交叉验证\names_train |>\n  vfold_cv(v = 10) -> ames_folds\names_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [2107/235]> Fold01\n 2 <split [2107/235]> Fold02\n 3 <split [2108/234]> Fold03\n 4 <split [2108/234]> Fold04\n 5 <split [2108/234]> Fold05\n 6 <split [2108/234]> Fold06\n 7 <split [2108/234]> Fold07\n 8 <split [2108/234]> Fold08\n 9 <split [2108/234]> Fold09\n10 <split [2108/234]> Fold10\n```\n\n\n:::\n:::\n\n\n#### 重复k折交叉验证repeated k-CV {.unnumbered}\n\n重复k折交叉验证是k折交叉验证的扩展，它重复k折交叉验证多次。重复k折交叉验证的优点是它提供了更准确的模型性能估计，但是计算成本更高。\n\n在`vfold_cv()`函数中，可以使用`repeats`参数指定重复次数。\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_train |>\n  vfold_cv(v = 10, repeats = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation repeated 5 times \n# A tibble: 50 × 3\n   splits             id      id2   \n   <list>             <chr>   <chr> \n 1 <split [2107/235]> Repeat1 Fold01\n 2 <split [2107/235]> Repeat1 Fold02\n 3 <split [2108/234]> Repeat1 Fold03\n 4 <split [2108/234]> Repeat1 Fold04\n 5 <split [2108/234]> Repeat1 Fold05\n 6 <split [2108/234]> Repeat1 Fold06\n 7 <split [2108/234]> Repeat1 Fold07\n 8 <split [2108/234]> Repeat1 Fold08\n 9 <split [2108/234]> Repeat1 Fold09\n10 <split [2108/234]> Repeat1 Fold10\n# ℹ 40 more rows\n```\n\n\n:::\n:::\n\n\n#### 留一法LOO {.unnumbered}\n\n留一法LOO是k折交叉验证的一个特例，其中k等于训练集的观测数。这时，每个评估集只包含一个观测。LOO的优点是它提供了最小的偏倚，但是计算成本很高。一般来说，LOO不适用于大型数据集。\n\n#### 蒙特卡罗交叉验证MCCV {.unnumbered}\n\n蒙特卡罗交叉验证是一种重复随机划分数据的方法。在每次重复中，数据被随机划分为训练集和评估集，且最终产生的评估集互斥。这种方法的优点是它可以提供更准确的模型性能估计，但是计算成本更高。\n\n`mc_cv()`函数可以用于创建蒙特卡罗交叉验证。其中，`prop`参数指定训练集的比例，`times`参数指定重复次数。\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_train |>\n  mc_cv(prop = 9/10, times = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Monte Carlo cross-validation (0.9/0.1) with 20 resamples  \n# A tibble: 20 × 2\n   splits             id        \n   <list>             <chr>     \n 1 <split [2107/235]> Resample01\n 2 <split [2107/235]> Resample02\n 3 <split [2107/235]> Resample03\n 4 <split [2107/235]> Resample04\n 5 <split [2107/235]> Resample05\n 6 <split [2107/235]> Resample06\n 7 <split [2107/235]> Resample07\n 8 <split [2107/235]> Resample08\n 9 <split [2107/235]> Resample09\n10 <split [2107/235]> Resample10\n11 <split [2107/235]> Resample11\n12 <split [2107/235]> Resample12\n13 <split [2107/235]> Resample13\n14 <split [2107/235]> Resample14\n15 <split [2107/235]> Resample15\n16 <split [2107/235]> Resample16\n17 <split [2107/235]> Resample17\n18 <split [2107/235]> Resample18\n19 <split [2107/235]> Resample19\n20 <split [2107/235]> Resample20\n```\n\n\n:::\n:::\n\n\n### 验证集\n\n验证集方法其实是只进行一次重采样，将数据分为训练集和验证集。验证集方法的优点是计算成本低，但是它提供的模型性能估计可能不准确。\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |>\n  initial_validation_split(prop = c(0.6, 0.2)) |> # 训练集60%，验证集20%，测试集20%\n  validation_set() # 验证集\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  splits             id        \n  <list>             <chr>     \n1 <split [1758/586]> validation\n```\n\n\n:::\n:::\n\n\n### Bootstrap\n\nBootstrap是一种重采样方法，它通过有放回地抽样来创建新的数据集。在每次重采样中，数据集的大小保持不变，但是每个观测可以被多次抽样。Bootstrap的优点是产生的性能估计方差较小，但是它可能产生较大的偏倚，尤其会低估准确率。\n\n`bootstraps()`函数可以用于创建Bootstrap。\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_train |>\n  bootstraps(times = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 10 × 2\n   splits             id         \n   <list>             <chr>      \n 1 <split [2342/881]> Bootstrap01\n 2 <split [2342/868]> Bootstrap02\n 3 <split [2342/896]> Bootstrap03\n 4 <split [2342/852]> Bootstrap04\n 5 <split [2342/861]> Bootstrap05\n 6 <split [2342/871]> Bootstrap06\n 7 <split [2342/852]> Bootstrap07\n 8 <split [2342/857]> Bootstrap08\n 9 <split [2342/857]> Bootstrap09\n10 <split [2342/880]> Bootstrap10\n```\n\n\n:::\n:::\n\n\n### 滚动抽样-时间序列\n\n滚动预测原点重采样方法是一种时间序列数据的重采样方法。初始训练集和评估集的大小是指定的。重采样的第一次迭代从序列的起点开始。第二次迭代向后移位一定数量的样本。流程如 @fig-rolling 所示。\n\n![滚动预测原点重采样](../images/rolling.svg){#fig-rolling}\n\n假设有一个时间序列数据集，由6组30天的数据块组成。可以设置初始训练集和评估集的大小为30天。第一次迭代，训练集包含第1-30天的数据，评估集包含第31-60天的数据。第二次迭代，训练集包含第31-60天的数据，评估集包含第61-90天的数据。以此类推。\n\n`rolling_origin()`函数可以用于创建滚动预测原点重采样。\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(x = 1:365) |>\n  rolling_origin(initial = 6 * 30, # 初始训练集大小\n                 assess = 30, # 评估集大小\n                 skip = 29, # 每次迭代的跳跃步长\n                 cumulative = FALSE) -> time_slices # 是否累积\n\ndata_range <- function(x) {\n  summarise(x, first = min(x), last = max(x))\n}\n\ntime_slices$splits |>\n  map_dfr(~ analysis(.x) |>\n            data_range())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  first  last\n  <int> <int>\n1     1   180\n2    31   210\n3    61   240\n4    91   270\n5   121   300\n6   151   330\n```\n\n\n:::\n\n```{.r .cell-code}\ntime_slices$splits |>\n  map_dfr(~ assessment(.x) |>\n            data_range())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  first  last\n  <int> <int>\n1   181   210\n2   211   240\n3   241   270\n4   271   300\n5   301   330\n6   331   360\n```\n\n\n:::\n:::\n\n\n## 评估重采样性能\n\n首先建立一个随机森林模型。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model <- \n  rand_forest(trees = 1000) |>\n  set_engine(\"ranger\") |>\n  set_mode(\"regression\")\n\nrf_workflow <- \n  workflow() |>\n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area +\n      Year_Built + Bldg_Type + Latitude + Longitude\n  ) |>\n  add_model(rf_model)\n\nrf_fit <- \n  rf_workflow |>\n  fit(data = ames_train)\n```\n:::\n\n\n### 十折交叉验证\n\n使用`control_resamples()`函数设置模型重采样控制参数；使用`fit_resamples()`函数对模型进行重采样训练和评估。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkeep_pred <-\n  control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\names_train |>\n  vfold_cv(v = 10) -> ames_folds\n\nset.seed(123)\nrf_workflow |>\n  fit_resamples(\n    resamples = ames_folds,\n    control = keep_pred\n  ) -> rf_res\n\nrf_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits             id     .metrics         .notes           .predictions\n   <list>             <chr>  <list>           <list>           <list>      \n 1 <split [2107/235]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 2 <split [2107/235]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 3 <split [2108/234]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 4 <split [2108/234]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 5 <split [2108/234]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 6 <split [2108/234]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 7 <split [2108/234]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 8 <split [2108/234]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 9 <split [2108/234]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n10 <split [2108/234]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n```\n\n\n:::\n:::\n\n\n在`rf_res`中，包含了每次重采样的模型性能评估结果。可以使用`collect_metrics()`函数提取模型性能指标。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0728    10 0.00261 Preprocessor1_Model1\n2 rsq     standard   0.830     10 0.0111  Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n方法的模型性能评估结果可以使用`collect_predictions()`函数提取。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_res |>\n  collect_predictions() -> assess_res\n\nassess_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2,342 × 5\n   id     .pred  .row Sale_Price .config             \n   <chr>  <dbl> <int>      <dbl> <chr>               \n 1 Fold01  5.07     1       5.10 Preprocessor1_Model1\n 2 Fold01  5.17    16       5.04 Preprocessor1_Model1\n 3 Fold01  4.91    27       4.90 Preprocessor1_Model1\n 4 Fold01  5.04    37       5.09 Preprocessor1_Model1\n 5 Fold01  5.02    40       5.04 Preprocessor1_Model1\n 6 Fold01  5.15    44       4.97 Preprocessor1_Model1\n 7 Fold01  5.12    57       5.10 Preprocessor1_Model1\n 8 Fold01  4.98    60       4.90 Preprocessor1_Model1\n 9 Fold01  4.92    69       5.01 Preprocessor1_Model1\n10 Fold01  5.08    78       4.88 Preprocessor1_Model1\n# ℹ 2,332 more rows\n```\n\n\n:::\n:::\n\n\n对性能评估结果`assess_res`进行可视化，如 @fig-resample_1 所示。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nassess_res |>\n  ggplot(aes(x = Sale_Price, y = .pred)) +\n  geom_point(alpha = 0.15) +\n  # 把x<4.5的点标记成红色\n  geom_point(data = filter(assess_res, Sale_Price < 4.5), color = \"red\", alpha = 0.25) +\n  geom_abline(color = \"red\") + \n  coord_obs_pred() +\n  theme_bw() +\n  ylab(\"Predicted\")\n```\n\n::: {.cell-output-display}\n![重采样性能评估结果(log10)](tidymodels-进阶_files/figure-html/fig-resample_1-1.png){#fig-resample_1 width=672}\n:::\n:::\n\n\n@fig-resample_1 中，标红的两个点表示销售价格较低的这两个房屋预测值大大偏高。可以在assess_res中定位到这两个数据，进而分析特定预测性能较差的可能原因。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nover_predicted <-\n  assess_res |>\n  mutate(residual = Sale_Price - .pred) |> # 计算残差\n  arrange(desc(abs(residual))) |> # 按残差绝对值降序排列\n  slice(1:2) # 取前两个数据\n\nover_predicted\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  id     .pred  .row Sale_Price .config              residual\n  <chr>  <dbl> <int>      <dbl> <chr>                   <dbl>\n1 Fold03  4.90   319       4.12 Preprocessor1_Model1   -0.783\n2 Fold06  4.87    32       4.11 Preprocessor1_Model1   -0.767\n```\n\n\n:::\n\n```{.r .cell-code}\names_train |>\n  slice(over_predicted$.row) |> # 取出这两个数据\n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath) # 选择感兴趣的变量\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n        <int> <fct>                       <int>         <int>     <int>\n1         733 Iowa_DOT_and_Rail_Road       1952             2         1\n2         832 Old_Town                     1923             2         1\n```\n\n\n:::\n:::\n\n\n### 验证集\n\n使用`fit_resamples()`函数对模型进行重采样训练和评估。\n\n\n::: {.cell}\n\n```{.r .cell-code}\names |>\n  initial_validation_split(prop = c(0.6, 0.2)) |>\n  validation_set() -> val_set # 划分验证集\n\nrf_workflow |>\n  fit_resamples(resamples = val_set) -> val_res # 对验证集进行重采样训练和评估\n\nval_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n#  \n# A tibble: 1 × 4\n  splits             id         .metrics         .notes          \n  <list>             <chr>      <list>           <list>          \n1 <split [1758/586]> validation <tibble [2 × 4]> <tibble [0 × 3]>\n```\n\n\n:::\n\n```{.r .cell-code}\ncollect_metrics(val_res) # 提取模型性能指标\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0657     1      NA Preprocessor1_Model1\n2 rsq     standard   0.845      1      NA Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n## 并行计算\n\n`tune`包使用`foreach`包来进行并行计算。\n\n`parallel`包可以计算本机的并行计算能力。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel::detectCores(logical = FALSE) # 物理核心数\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n\n```{.r .cell-code}\nparallel::detectCores(logical = TRUE) # 逻辑核心数，包括超线程\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 16\n```\n\n\n:::\n:::\n\n\n`doParallel`包可以使用`registerDoParallel()`函数注册并行计算。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doParallel)\n\ncl <- makePSOCKcluster(4) # 创建4个核心的并行计算集群\nregisterDoParallel(cl) # 注册并行计算\n\n## 运行fit_resamples()函数----------------\n\nstopCluster(cl) # 关闭并行计算集群\n```\n:::\n\n\n## 利用重采样方法比较模型\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls())\nload(\"../data/tidymodels_2_1.rda\")\n```\n:::\n\n\n### 建立多个模型\n\n建立三个不同的线性回归模型。\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 基本模型----------------\nbasic_rec <-\n  ames_train |>\n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude) |>\n  step_log(Gr_Liv_Area, base = 10) |>\n  step_other(Neighborhood, threshold = 0.01) |>\n  step_dummy(all_nominal_predictors())\n\n## 基本模型 + 交互项----------------\ninteraction_rec <-\n  basic_rec |>\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\"))\n\n## 基本模型 + 交互项 + 自然样条----------------\nspline_rec <- \n  interaction_rec |> \n  step_ns(Latitude, Longitude, deg_free = 50)\n\n## 建立模型----------------\npreproc <- \n  list(basic = basic_rec, \n       interact = interaction_rec, \n       splines = spline_rec\n  )\n\nlm_models <-\n  preproc |>\n  workflow_set(list(lm = linear_reg()), cross = FALSE) # 建立线性回归模型，不进行重采样\n\nlm_models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A workflow set/tibble: 3 × 4\n  wflow_id    info             option    result    \n  <chr>       <list>           <list>    <list>    \n1 basic_lm    <tibble [1 × 4]> <opts[0]> <list [0]>\n2 interact_lm <tibble [1 × 4]> <opts[0]> <list [0]>\n3 splines_lm  <tibble [1 × 4]> <opts[0]> <list [0]>\n```\n\n\n:::\n:::\n\n\n使用`workflow_map()`函数对三个模型进行重采样。`verbose = TRUE`参数可以显示进度条，`seed = 123`参数可以设置随机种子。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_models <-\n  lm_models |>\n  workflow_map(\n    \"fit_resamples\",\n    seed = 123,\n    verbose = TRUE,\n    resamples = ames_folds,\n    control = keep_pred\n  )\n\nlm_models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A workflow set/tibble: 3 × 4\n  wflow_id    info             option    result   \n  <chr>       <list>           <list>    <list>   \n1 basic_lm    <tibble [1 × 4]> <opts[2]> <rsmp[+]>\n2 interact_lm <tibble [1 × 4]> <opts[2]> <rsmp[+]>\n3 splines_lm  <tibble [1 × 4]> <opts[2]> <rsmp[+]>\n```\n\n\n:::\n:::\n\n\n使用`collect_metrics()`函数提取模型性能指标。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_models |>\n  collect_metrics() |>\n  filter(.metric == \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 9\n  wflow_id    .config      preproc model .metric .estimator   mean     n std_err\n  <chr>       <chr>        <chr>   <chr> <chr>   <chr>       <dbl> <int>   <dbl>\n1 basic_lm    Preprocesso… recipe  line… rmse    standard   0.0794    10 0.00318\n2 interact_lm Preprocesso… recipe  line… rmse    standard   0.0789    10 0.00320\n3 splines_lm  Preprocesso… recipe  line… rmse    standard   0.0777    10 0.00308\n```\n\n\n:::\n:::\n\n\n添加其他模型时，需要提前在其他模型重采样流程中设置`save_workflow = TRUE`。使用`as_workflow_set()`函数将保存的工作流转换为`workflow_set`对象。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfour_models <-\n  as_workflow_set(random_forest = rf_res) |>\n  bind_rows(lm_models)\n\nfour_models\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A workflow set/tibble: 4 × 4\n  wflow_id      info             option    result   \n  <chr>         <list>           <list>    <list>   \n1 random_forest <tibble [1 × 4]> <opts[0]> <rsmp[+]>\n2 basic_lm      <tibble [1 × 4]> <opts[2]> <rsmp[+]>\n3 interact_lm   <tibble [1 × 4]> <opts[2]> <rsmp[+]>\n4 splines_lm    <tibble [1 × 4]> <opts[2]> <rsmp[+]>\n```\n\n\n:::\n:::\n\n\n使用`autoplot()`函数对模型进行可视化。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggrepel)\nfour_models |>\n  autoplot(metric = \"rsq\") + # 可视化R^2\n  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) + # 添加模型名称\n  theme(legend.position = \"none\") # 隐藏图例\n```\n\n::: {.cell-output-display}\n![](tidymodels-进阶_files/figure-html/fig-model_1-1.png){#fig-model_1 width=672}\n:::\n:::\n\n\n由 @fig-model_1 可以看出，随机森林模型的R^2^值最高，且随着模型复杂度的增加，R^2^值也在增加，说明线性模型有细微的改进。\n\n### 比较重采样的性能统计\n\n在不同的线性模型之间，R^2^值的差异并不大。但是，这种差异是否具有统计学意义仍需进一步检验。\n\n#### 假设检验\n\n可以用配对t检验方法检验不同模型之间的R^2^值差异是否具有统计学意义。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq_indiv_estimates <-\n  four_models |>\n  collect_metrics(summarize = FALSE) |> # 提取模型性能指标\n  filter(.metric == \"rsq\") # 提取R^2^值\n\nrsq_wider <-\n  rsq_indiv_estimates |>\n  select(wflow_id, .estimate, id) |>\n  pivot_wider(id_cols = \"id\", names_from = \"wflow_id\", values_from = \".estimate\")\n\ncorrr::correlate(rsq_wider %>% select(-id), quiet = TRUE) # 计算R^2^值的相关系数\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term          random_forest basic_lm interact_lm splines_lm\n  <chr>                 <dbl>    <dbl>       <dbl>      <dbl>\n1 random_forest        NA        0.925       0.933      0.915\n2 basic_lm              0.925   NA           0.996      0.977\n3 interact_lm           0.933    0.996      NA          0.977\n4 splines_lm            0.915    0.977       0.977     NA    \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 基本模型和交互模型-------------------------------------------------------\nrsq_wider |>\n  with(t.test(splines_lm, basic_lm, paired = TRUE)) |>\n  tidy() |>\n  select(estimate, p.value, starts_with(\"conf\")) # 提取估计值、p值和置信区间\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  estimate p.value conf.low conf.high\n     <dbl>   <dbl>    <dbl>     <dbl>\n1  0.00770  0.0391 0.000480    0.0149\n```\n\n\n:::\n:::\n\n\n根据假设检验结果，p值不显著，且R^2^值差异仅为0.77%。因此，不同模型之间的R^2^值差异不具有统计学意义。\n\n#### 贝叶斯\n\n##### 随机截距模型\n\n使用`tidyposterior`包中的`perf_mod()`函数可以建立贝叶斯模型，并将其与重采样统计量拟合。\n\n\n::: {.cell}\n\n```{.r .cell-code}\npackages <- c(\"tidyposterior\", \"rstanarm\")\nfor (pkg in packages) {\n  if (!require(pkg, character.only = TRUE, quietly = TRUE)) {\n    install.packages(pkg, dependencies = TRUE)\n    require(pkg, character.only = TRUE, quietly = TRUE)\n  }\n}\n\n# 建立贝叶斯先验模型--------------------------------------------------------\nrsq_anova <-\n  four_models |>\n  perf_mod(\n    metric = \"rsq\",\n    prior_intercept = rstanarm::student_t(df = 1), # 指定拟合模型时用于截距项的先验分布\n    chains = 4,\n    iter = 5000,\n    seed = 123,\n    refresh = 0 # 不显示进度条\n  )\n\n# 提取模型后验信息-----------------------------------------------------------\nmodel_post <-\n  rsq_anova |>\n  tidy(seed = 1103)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_post |>\n  mutate(model = forcats::fct_inorder(model)) |> # 对模型名称进行排序\n  ggplot(aes(x = posterior)) +\n  geom_histogram(bins = 50, color = \"white\", fill = \"blue\", alpha = 0.4) +\n  facet_wrap(~ model, ncol = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![后验分布](tidymodels-进阶_files/figure-html/fig-bayes_1-1.png){#fig-bayes_1 width=672}\n:::\n:::\n\n\n@fig-bayes_1 和 @fig-bayes_2 分别展示了不同模型的平均R^2^值的估计概率分布和置信区间。可以看出，各个模型的后验分布有所重叠，尤其是三个线性模型之间，说明不同模型之间的R^2^值差异不具有统计学意义。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq_anova |>\n  autoplot() +\n  geom_text_repel(aes(label = workflow), nudge_x = 1/8, nudge_y = 1/100) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![后验分布的置信区间](tidymodels-进阶_files/figure-html/fig-bayes_2-1.png){#fig-bayes_2 width=672}\n:::\n:::\n\n\n使用`contrast_models()`函数对不同模型的R^2^差异的后验分布进行比较。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq_diff <-\n  rsq_anova |>\n  contrast_models(\n    list_1 = \"splines_lm\",\n    list_2 = \"basic_lm\",\n    seed = 123\n  )\n\nrsq_diff |>\n  as_tibble() |>\n  ggplot(aes(x = difference)) +\n  geom_vline(xintercept = 0, lty = 2) + \n  geom_histogram(bins = 50, color = \"white\", fill = \"red\", alpha = 0.4)\n```\n\n::: {.cell-output-display}\n![决定系数差异的后验分布](tidymodels-进阶_files/figure-html/fig-bayes_3-1.png){#fig-bayes_3 width=672}\n:::\n:::\n\n\n如 @fig-bayes_3 所示，不同模型之间的R^2^值差异的后验分布均值接近于0，且置信区间包含0，说明不同模型之间的R^2^值差异不具有统计学意义。可以进一步使用`summary`函数计算分布的平均值以及可信区间。其中，`probability`表示差异大于0的概率，`mean`表示差异的平均值。\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq_diff |>\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 9\n  contrast        probability    mean   lower  upper  size pract_neg pract_equiv\n  <chr>                 <dbl>   <dbl>   <dbl>  <dbl> <dbl>     <dbl>       <dbl>\n1 splines_lm vs …       0.990 0.00768 0.00232 0.0130     0        NA          NA\n# ℹ 1 more variable: pract_pos <dbl>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq_anova |>\n  autoplot(type = \"ROPE\", size = 0.02) +\n  geom_text_repel(aes(label = workflow)) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![效应大小为 2% 的实际等效概率](tidymodels-进阶_files/figure-html/fig-bayes_4-1.png){#fig-bayes_4 width=672}\n:::\n:::\n\n\n:::callout-tip\n重采样的次数越多，后验分布的形状越接近于正态分布。因此，可以通过增加重采样次数来提高模型的稳定性。\n:::\n\n## 模型参数调优\n\n\n",
    "supporting": [
      "tidymodels-进阶_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}